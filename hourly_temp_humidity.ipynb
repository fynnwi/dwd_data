{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Data from Deutscher Wetterdienst (DWD)\n",
    "CDC = Climate Data Center\n",
    "\n",
    "We are only going to look at the **recent** data provided by DWD, which includes the last 500 days until yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from zipfile import ZipFile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"retrieved_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_hourly_recent = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent/\"\n",
    "URL_station_info = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent/TU_Stundenwerte_Beschreibung_Stationen.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the list of weather stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_weather_station_info(text):\n",
    "    data = []\n",
    "    for line in text[2:]:\n",
    "        e = line.split()\n",
    "        station_id = e.pop(0)\n",
    "        start_date = e.pop(0)\n",
    "        end_date = e.pop(0)\n",
    "        altitude = e.pop(0)\n",
    "        latitude = e.pop(0)\n",
    "        longitude = e.pop(0)\n",
    "        state = e.pop(-1)\n",
    "        station_name = \" \".join(e)\n",
    "\n",
    "        row = [station_id, start_date, end_date, altitude, latitude, longitude, station_name, state]\n",
    "        data.append(row)\n",
    "\n",
    "    columns = [\"station_id\", \"start_date\", \"end_date\", \"altitude\", \"latitude\", \"longitude\", \"name\", \"state\"]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "req = requests.get(URL_station_info)\n",
    "lines = req.text.splitlines()\n",
    "df = parse_weather_station_info(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape all zip links that are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_product_links(soup):\n",
    "    anchors = soup.find_all(\"a\")\n",
    "    links = []\n",
    "    \n",
    "    for a in anchors:\n",
    "        ref = a.get(\"href\")\n",
    "        if ref.startswith(\"stundenwerte_TU_\") and ref.endswith(\"_akt.zip\"):\n",
    "            links.append(ref)\n",
    "    return links\n",
    "\n",
    "req = requests.get(URL_hourly_recent)\n",
    "soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "file_urls = scrape_product_links(soup)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download all zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 of 508 files downloaded\n",
      "100 of 508 files downloaded\n",
      "150 of 508 files downloaded\n",
      "200 of 508 files downloaded\n",
      "250 of 508 files downloaded\n",
      "300 of 508 files downloaded\n",
      "350 of 508 files downloaded\n",
      "400 of 508 files downloaded\n",
      "450 of 508 files downloaded\n",
      "500 of 508 files downloaded\n",
      "508 of 508 files downloaded\n"
     ]
    }
   ],
   "source": [
    "def download_all_product_files(file_urls):\n",
    "    total = len(file_urls)\n",
    "    for i, url in enumerate(file_urls, start=1):\n",
    "        req = requests.get(URL_hourly_recent + url)\n",
    "\n",
    "        with open(DATA_DIR + url, \"wb\") as file:\n",
    "            file.write(req.content)\n",
    "            \n",
    "        # unzip the file and only keep the extracted content\n",
    "        with ZipFile(DATA_DIR + url, \"r\") as zippy:\n",
    "            dirname = DATA_DIR + url[:-4]\n",
    "            try:\n",
    "                os.mkdir(dirname)\n",
    "                zippy.extractall(dirname)\n",
    "                os.remove(DATA_DIR + url)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        \n",
    "        if i % 50 == 0 or i == total:\n",
    "            print(\"%d of %d files downloaded\" % (i , total))\n",
    "    \n",
    "download_all_product_files(file_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract temperature und humidity data from all station data\n",
    "Now, for every available weather station there exists a folder containing measurement as well as meta data about the corresponding station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
